{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# from gensim.models import KeyedVectors\n",
    "# import gensim.downloader as api\n",
    "# from huggingface_hub import hf_hub_download\n",
    "\n",
    "# NLP\n",
    "import string, re, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The objective of the project is to classify a product into the four categories \n",
    "# Electronics, Household, Books and Clothing & Accessories, \n",
    "# based on its description available in the e-commerce platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source : https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification\n",
    "# Code : https://www.kaggle.com/code/sugataghosh/e-commerce-text-classification-tf-idf-word2vec#Text-Normalization \n",
    "\n",
    "data = pd.read_csv('data/ecommerceDataset.csv', names = ['category','description'], header = None)\n",
    "print(data['category'].value_counts())\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values and duplicate observations\n",
    "print(pd.Series({\"Number of observations with missing values\": len(data) - len(data.dropna()),\n",
    "                 \"Number of duplicate observations\": data.duplicated().sum()}).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace = True) # Dropping observations with missing values\n",
    "data.drop_duplicates(inplace = True) # Dropping duplicate observations\n",
    "data.reset_index(drop = True, inplace = True) # Resetting index\n",
    "\n",
    "# Manual encoding of labels\n",
    "label_dict = {'Electronics': 0, 'Household': 1, 'Books': 2, 'Clothing & Accessories': 3}\n",
    "data = data.replace({'category': label_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-target split\n",
    "X, y = data.drop('category', axis = 1), data['category']\n",
    "\n",
    "# Train-test split (from complete data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 40)\n",
    "\n",
    "# Validation-test split (from test data)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In natural language processing, **text normalization** is the process of transforming text into a single canonical form. We consider a number of text normalization processes. At the end of the section, we combine selected processes into one single function and apply it on the product descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming and Lemmatization**\n",
    "-  Stemming is the process of reducing the words to their root form or stem. It reduces related words to the same stem even if the stem is not a dictionary word. For example, the words introducing, introduced, introduction reduce to a common word introduce. However, the process often produces stems that are not actual words.\n",
    "  \n",
    "- Lemmatization offers a more sophisticated approach by utilizing a corpus to match root forms of the words. Unlike stemming, it uses the context in which a word is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to lowercase\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Removing whitespaces\n",
    "def remove_whitespace(text):\n",
    "    return text.strip()\n",
    "\n",
    "# Remove Punctuation\n",
    "def remove_punctuation(text):\n",
    "    punct_str = string.punctuation # contains a predefined set of punctuation characters.\n",
    "    punct_str = punct_str.replace(\"'\", \"\") # discarding apostrophe from the string\n",
    "    clean_text = \"\".join(char for char in text if char not in punct_str)\n",
    "    return clean_text\n",
    "\n",
    "# Remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    regexp = RegexpTokenizer(\"[\\w']+\")\n",
    "    \n",
    "    stops = stopwords.words(\"english\") # stopwords\n",
    "    clean_text = \" \".join([word for word in regexp.tokenize(text) if word not in stops])\n",
    "    return clean_text\n",
    "\n",
    "# Lemmatization\n",
    "spacy_lemmatizer = spacy.load(\"en_core_web_sm\", disable = ['parser', 'ner'])\n",
    "def text_lemmatizer(text):\n",
    "    text_spacy = \" \".join([token.lemma_ for token in spacy_lemmatizer(text)])\n",
    "    return text_spacy\n",
    "\n",
    "def text_normalizer(text):\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = text_lemmatizer(text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['description'] = X_train['description'].apply(text_normalizer)\n",
    "X_val['description'] = X_val['description'].apply(text_normalizer)\n",
    "X_test['description'] = X_test['description'].apply(text_normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF (short for term frequency-inverse document frequency), is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Corpus contains multiple documents, document contains multiple terms\n",
    "- Corpus = Collection of Sentences\n",
    "- Document = Sentence\n",
    "- Term = Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Term frequency (TF) is the relative frequency of a term within a given document. It is obtained as the number of times a word appears in a text, divided by the total number of words appearing in the text.**\n",
    "  \n",
    "- **Inverse document frequency (IDF) measures how common or rare a word is across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that ratio.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Term Frequency (TF) Formula:**  \n",
    "\n",
    "$$\n",
    "TF(t, d) = \\frac{f_t}{N}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ f_t $ = Number of times term $ t $ appears in document $ d $  \n",
    "- $ N $ = Total number of terms in document $ d $  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Inverse Document Frequency (IDF) Formula:**  \n",
    "\n",
    "$$\n",
    "IDF(t) = \\log \\left(\\frac{N_d}{N_t + 1} \\right)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ N_d $ = Total number of documents in the corpus  \n",
    "- $ N_t $ = Number of documents that contain term $ t $  \n",
    "- The \"+1\" in the denominator is used to prevent division by zero (i.e., smoothing).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **TF-IDF Formula:**  \n",
    "\n",
    "$$\n",
    "TF\\text{-}IDF(t, d) = TF(t, d) \\times IDF(t)\n",
    "$$\n",
    "\n",
    "This helps in weighting terms based on their importance in a document relative to the entire corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorization\n",
    "TfidfVec = TfidfVectorizer()\n",
    "X_train_tfidf = TfidfVec.fit_transform(X_train[\"description\"])\n",
    "X_val_tfidf = TfidfVec.transform(X_val[\"description\"])\n",
    "X_test_tfidf = TfidfVec.transform(X_test[\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. \n",
    "- For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "y_test_pred, y_val_pred = model.predict(X_test_tfidf), model.predict(X_val_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute Macro F1-score\n",
    "f1_macro = f1_score(y_test, y_test_pred, average='macro')\n",
    "print(f\"Macro F1-score: {f1_macro:.4f}\")\n",
    "\n",
    "# 2. Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# 3. Compute Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/anand.thirwani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "Household                 19313\n",
      "Books                     11820\n",
      "Electronics               10621\n",
      "Clothing & Accessories     8671\n",
      "Name: count, dtype: int64\n",
      "(50425, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j2/2p1vsnfj2kn0wrq64fqdvds448_r7g/T/ipykernel_63696/4188303982.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data = data.replace({'category': label_dict})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Paper Plane Design Framed Wall Hanging Motivat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SAF 'Floral' Framed Painting (Wood, 30 inch x ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>SAF 'UV Textured Modern Art Print Framed' Pain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>SAF Flower Print Framed Painting (Synthetic, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Incredible Gifts India Wooden Happy Birthday U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                        description\n",
       "0         1  Paper Plane Design Framed Wall Hanging Motivat...\n",
       "1         1  SAF 'Floral' Framed Painting (Wood, 30 inch x ...\n",
       "2         1  SAF 'UV Textured Modern Art Print Framed' Pain...\n",
       "3         1  SAF Flower Print Framed Painting (Synthetic, 1...\n",
       "4         1  Incredible Gifts India Wooden Happy Birthday U..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/ecommerceDataset.csv', names = ['category','description'], header = None)\n",
    "print(data['category'].value_counts())\n",
    "print(data.shape)\n",
    "\n",
    "data.dropna(inplace = True) # Dropping observations with missing values\n",
    "data.drop_duplicates(inplace = True) # Dropping duplicate observations\n",
    "data.reset_index(drop = True, inplace = True) # Resetting index\n",
    "\n",
    "# Manual encoding of labels\n",
    "label_dict = {'Electronics': 0, 'Household': 1, 'Books': 2, 'Clothing & Accessories': 3}\n",
    "data = data.replace({'category': label_dict})\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-target split\n",
    "X, y = data['description'].values, data['category'].values\n",
    "\n",
    "# Train-test split (from complete data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 40)\n",
    "\n",
    "# Validation-test split (from test data)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 5000  # Maximum vocabulary size\n",
    "max_length = 100  # Maximum sentence length\n",
    "embedding_dim = 128  # Word embedding dimensions\n",
    "\n",
    "# Tokenization\n",
    "# Builds the vocabulary based on X_train. Assigns an index (number) to each word\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\") # tensorflow tokeniser\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "# This replaces words with their assigned indices (numbers).\n",
    "# if X_train = [\"I love this movie\", \"This movie is amazing\"], then X_train_seq = [[3, 4, 1, 2],  [1, 2, 5, 6]]\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anand.thirwani/Documents/Study/AI_ML_Popular_Codes/venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define LSTM Model\n",
    "inputs = layers.Input(shape=(max_length,))\n",
    "embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(embedding)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Bidirectional(layers.LSTM(32, return_sequences=False))(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "# Compile Model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss',  # Monitor validation loss\n",
    "                               patience=3,          # Stop after 3 epochs of no improvement\n",
    "                               restore_best_weights=True,  # Restore best weights\n",
    "                               verbose=1)\n",
    "history = model.fit(X_train_pad, y_train, epochs=20, batch_size=4, validation_data=(X_val_pad, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since softmax is used, the output will be a probability distribution across the 4 classes.\n",
    "y_test_pred = model.predict(X_test_pad)\n",
    "print(y_test_pred)\n",
    "\n",
    "# To get the predicted class, take the argmax (index of the highest probability):\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute Macro F1-score\n",
    "f1_macro = f1_score(y_test, y_test_pred, average='macro')\n",
    "print(f\"Macro F1-score: {f1_macro:.4f}\")\n",
    "\n",
    "# 2. Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# 3. Compute Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n",
      "Is MPS available?: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Is MPS available?:\", tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Practical Approach To Acupuncture: 1 About the Author The author graduated in allopathy [MBBS] in 1954. She went on to acquire a postgraduate diploma in gynecology and obstetrics, and went into General practice in Mumbai, that kept her busy for 30 years. She realized that allopathy had its limit, creating a deadlockin the treatment of certain disorders.',\n",
       "       'Nice Goods Leatherette Office Arm Chair (Brown) This Chair Has Adjustable Seat Height, Wheels, Armrest, Swivel. Ideal For Home & Office Use.',\n",
       "       'Ekan Fashionable Fedora Hat for Girls, Boys Fedora Hat, Hats for Men Stylish Casual, Fedora Hat for Women Red Color 30Gram (Pack of 1) Fedora Hats For Men And Women Stylish(Ekan) Stylish Hat with elegant designs and hues designed with combination of great quality and fashion. Complete your look with this Red fedora hat from Ekan. Made from high quality material, this hat will redefine your casual look and make you look all the more stylish',\n",
       "       ...,\n",
       "       \"The Challenger Sale: Taking Control of the Customer Conversation Review “The history of sales has been one of steady progress interrupted by a few real breakthroughs that have changed the whole direction of the pro\\xadfession. These breakthroughs, marked by radical new thinking and dra\\xadmatic improvements in sales results, have been rare. . . . Which brings me to The Challenger Sale and the work of the Sales Executive Council. . . . On the face of it, their research has all the initial signs that it may be game-changing. . . . My advice is this: Read it, think about it, implement it. You, and your organization, will be glad you did.”—Professor Neil Rackham, author of SPIN Selling, from the foreword “The amazing thing is that the Challenger sales rep has been hiding in plain sight all these years. The Challenger Sale breaks the winning elements of this powerful approach into a set of teachable skills that can take even a top sales team to a new level of results delivery.”—Dan James, former chief sales officer, DuPont \\xa0“This is a must-read book for every sales professional. The authors’ groundbreak\\xading research explains how the rules for selling have changed—and what to do about it. If you don’t want to be left behind, don’t miss this innovative book that provides the new formula for selling success.”—Ken Revenaugh, vice president, sales operations, Oakwood Temporary Housing “Groundbreaking, timely, and disciplined research—presented in a way that is both intuitive and completely actionable—that has already had an impact on our organization by creating a customer lens that enhanced our sales recruiting, hiring, training, and deployment.”—Jeff Connor, senior vice president and chief growth officer, ARAMARK Global Food, Hospitality and Facility Services “The Challenger Sale shows you how to maintain control of the complex sale. The output of this superbly researched body of work is that you will know how to better differentiate your organization, your offering, and yourself in the mind of the customer.”—Adrian Norton, vice president, sales, Reckitt Benckiser Pharmaceuticals “There is a healthy dose of constructive tension throughout this brilliant book. Tension that will bring insight and clarity into how customers buy today and how your sales team must sell. If you are seeking to raise the bar in your sales orga\\xadnization, The Challenger Sale is a must-read.”—Tom Meek, vice president, sales, Henkel Adhesives Technologies About the Author Matthew Dixon is a managing director and Brent Adamson is a senior director with Corporate Executive Board's Sales Executive Council in Washington, D.C. About Corporate Executive Board By identifying and building on the proven best practices of the world's best companies, Corporate Executive Board (CEB) helps senior executives and their teams drive corporate performance. CEB tools, insights, and analysis empower clients to focus efforts, move quickly, and address emerging and enduring business challenges with confidence. For more information visit www.executiveboard.com www.thechallengersale.com\",\n",
       "       'International Mathematics Olympiad Work Book (IMO) - Class 7 for 2018-19 ',\n",
       "       \"Operating System Concepts:8th Edition Wiley Student Edition About the Author About the Author: Abraham Silberschatz is the Sidney J. Weinberg Professor and Chair of Computer Science at Yale University. He served as the Vice President of the Information Sciences Research Center at Bell Laboratories before joining Yale. Before that, he was a professor at the University of Texas at Austin in the Department of Computer Sciences. He is also a part of IEEE and ACM. He received the ACM SIGMOD Contribution Award in 1997. in 1998, he was awarded the ACM Karl V. Karlstrom Outstanding Educator Award and The IEEE Taylor L. Booth Education Award was awarded to Mr Silberchatz in the 2002. He was awarded the Bell Laboratories President's Award for his innovative technical excellence. Famous in his field of teachings, he also teaches computer networks, distributed systems and software engineering. He also provides workshops to computer science educators and industry professionals. Being a writer, Professor Silberschatz' works have appeared and highlighted in ACM and IEEE publications and several other journals and conferences.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(32)\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 109485316\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum([tf.keras.backend.count_params(p) for p in model.trainable_variables])\n",
    "print(f\"Total Trainable Parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all BERT layers\n",
    "model.bert.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 3076\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum([tf.keras.backend.count_params(p) for p in model.trainable_variables])\n",
    "print(f\"Total Trainable Parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x30cd3f130> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x30cd3f130> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "696/696 [==============================] - 761s 1s/step - loss: 1.4046 - accuracy: 0.2293\n",
      "Epoch 2/2\n",
      "696/696 [==============================] - 707s 1s/step - loss: 1.3863 - accuracy: 0.2241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x355c2a620>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 30s 349ms/step\n",
      "[[0.15812528 0.00661241 0.64668757 0.18857472]\n",
      " [0.1581253  0.00661241 0.6466877  0.1885747 ]\n",
      " [0.1581253  0.00661241 0.6466877  0.1885747 ]\n",
      " ...\n",
      " [0.1581253  0.00661241 0.6466876  0.1885747 ]\n",
      " [0.1581253  0.00661241 0.6466876  0.1885747 ]\n",
      " [0.1581253  0.00661241 0.6466877  0.1885747 ]]\n"
     ]
    }
   ],
   "source": [
    "# Get raw predictions\n",
    "predictions = model.predict(test_dataset).logits  # Extract logits\n",
    "# Convert logits to class probabilities\n",
    "probabilities = tf.nn.softmax(predictions, axis=1).numpy()\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# To get the predicted class, take the argmax (index of the highest probability).\n",
    "y_test_pred = np.argmax(probabilities, axis=1)\n",
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1-score: 0.0940\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       525\n",
      "           1       0.00      0.00      0.00      1056\n",
      "           2       0.23      1.00      0.38       644\n",
      "           3       0.00      0.00      0.00       556\n",
      "\n",
      "    accuracy                           0.23      2781\n",
      "   macro avg       0.06      0.25      0.09      2781\n",
      "weighted avg       0.05      0.23      0.09      2781\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0    0  525    0]\n",
      " [   0    0 1056    0]\n",
      " [   0    0  644    0]\n",
      " [   0    0  556    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anand.thirwani/Documents/Study/AI_ML_Popular_Codes/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/anand.thirwani/Documents/Study/AI_ML_Popular_Codes/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/anand.thirwani/Documents/Study/AI_ML_Popular_Codes/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# 1. Compute Macro F1-score\n",
    "f1_macro = f1_score(y_test, y_test_pred, average='macro')\n",
    "print(f\"Macro F1-score: {f1_macro:.4f}\")\n",
    "\n",
    "# 2. Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# 3. Compute Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "# Word 2 Vec\n",
    "# BiLSTM\n",
    "# Conv 1D\n",
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def SGD(X, y, gradient_function, num_epochs, lr):\n",
    "#     num_rows, num_cols = X.shape[0], X.shape[1]\n",
    "#     theta = np.random.randn(num_cols)\n",
    "\n",
    "#     for e in range(num_epochs):\n",
    "#         for i in range(num_rows):\n",
    "#             xi = X[i]\n",
    "#             yi = y[i]\n",
    "#             grad = gradient_function(xi, yi, theta)\n",
    "#             theta = theta - (lr * grad)\n",
    "#     return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Loss  | Loss Function (Matrix Form) | Gradient |\n",
    "|-------|-----------------------------|------------------------------------------------------|\n",
    "| **MSE** | $$\\mathcal{L}_{MSE} = \\frac{1}{N} \\| Y - X\\theta \\|_2^2$$ | $$\\frac{\\partial \\mathcal{L}_{MSE}}{\\partial \\theta} = -\\frac{2}{N} X^T (Y - X\\theta)$$ |\n",
    "| **BCE** | $$\\mathcal{L}_{BCE} = -\\frac{1}{N} \\left[ Y^T \\log \\sigma(X\\theta) + (1 - Y)^T \\log (1 - \\sigma(X\\theta)) \\right]$$ | $$\\frac{\\partial \\mathcal{L}_{BCE}}{\\partial \\theta} = -\\frac{1}{N} X^T (Y - \\sigma(X\\theta))$$ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def MBGD(X, y, gradient_function, num_epochs, lr, batch_size):\n",
    "    num_rows, num_cols = X.shape[0], X.shape[1]\n",
    "    theta = np.random.randn(num_cols)\n",
    "\n",
    "    indices = np.random.permutation(len(X)) \n",
    "    X, y = X[indices], y[indices]\n",
    "\n",
    "    # no_improve_count = 0  # Counter for stopping patience\n",
    "    # tol = 1e-2\n",
    "    # patience = 5\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "        for b in range(0, num_rows, batch_size):\n",
    "            X_batch = X[b:b+batch_size]\n",
    "            y_batch = y[b:b+batch_size].flatten()\n",
    "            grad = gradient_function(X_batch, y_batch, theta)\n",
    "            theta = theta - (lr * grad)\n",
    "\n",
    "            # # **Stopping Criterion: Check gradient magnitude**\n",
    "            # grad_norm = np.linalg.norm(grad)  # Compute L2 norm of gradient\n",
    "            # if grad_norm < tol:\n",
    "            #     no_improve_count += 1\n",
    "            # else:\n",
    "            #     no_improve_count = 0  # Reset if gradient is significant\n",
    "\n",
    "            # # **Early stopping if gradient is too small for `patience` epochs**\n",
    "            # if no_improve_count >= patience:\n",
    "            #     print(f\"Stopping early at epoch {e+1} due to small gradient updates.\")\n",
    "            #     return theta\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.99886433, 3.00427567])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse_grad(X, y, theta):\n",
    "    n = len(y) \n",
    "    return (-2 / n) * X.T @ (y - (X @ theta))\n",
    "\n",
    "X = np.random.randn(10000, 1)\n",
    "noise = np.random.randn(10000, 1) * 0.2  # Add noise\n",
    "y = 4 + 3*X + noise\n",
    "X = np.column_stack([np.ones(10000), X])\n",
    "\n",
    "MBGD(X, y, mse_grad, 1000, 0.01, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.57146676, 2.63263087])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def bce_grad(X, y, theta):\n",
    "    n = len(y) \n",
    "    return (-1 / n) * X.T @ (y - sigmoid(X @ theta))\n",
    "\n",
    "X = np.random.randn(10000, 1)\n",
    "noise = np.random.randn(10000, 1) * 0.2  # Add noise\n",
    "y = sigmoid(4 + 3*X + noise) \n",
    "# y = (y > 0.5).astype(int)\n",
    "X = np.column_stack([np.ones(10000), X])\n",
    "\n",
    "MBGD(X, y, bce_grad, 50, 0.01, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Squared Error (MSE)\n",
    "Loss Function\n",
    "For predictions ŷ = f(X;θ) and true values y:\n",
    "$$\\mathcal{L}{MSE}(\\theta) = \\frac{1}{n} \\sum{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} |y - \\hat{y}|^2_2$$\n",
    "In matrix form:\n",
    "$$\\mathcal{L}_{MSE}(\\theta) = \\frac{1}{n} (y - \\hat{y})^T(y - \\hat{y})$$\n",
    "Gradient\n",
    "For a linear model where ŷ = Xθ:\n",
    "$$\\nabla_\\theta\\mathcal{L}_{MSE}(\\theta) = -\\frac{2}{n}X^T(y - X\\theta)$$\n",
    "For a general model where ŷ = f(X;θ):\n",
    "$$\\nabla_\\theta\\mathcal{L}_{MSE}(\\theta) = -\\frac{2}{n}J^T(y - \\hat{y})$$\n",
    "where J is the Jacobian matrix with elements $J_{ij} = \\frac{\\partial \\hat{y}_i}{\\partial \\theta_j}$\n",
    "Binary Cross-Entropy (BCE)\n",
    "Loss Function\n",
    "For binary classification with predicted probabilities ŷ = σ(f(X;θ)) where σ is the sigmoid function:\n",
    "$$\\mathcal{L}{BCE}(\\theta) = -\\frac{1}{n}\\sum{i=1}^{n} \\left[ y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i) \\right]$$\n",
    "In matrix form:\n",
    "$$\\mathcal{L}_{BCE}(\\theta) = -\\frac{1}{n}\\left[ y^T\\log(\\hat{y}) + (1-y)^T\\log(1-\\hat{y}) \\right]$$\n",
    "Gradient\n",
    "For a linear model where ŷ = σ(Xθ):\n",
    "$$\\nabla_\\theta\\mathcal{L}_{BCE}(\\theta) = \\frac{1}{n}X^T(\\hat{y} - y)$$\n",
    "For a general model where ŷ = σ(f(X;θ)):\n",
    "$$\\nabla_\\theta\\mathcal{L}_{BCE}(\\theta) = \\frac{1}{n}J^T(\\hat{y} - y)$$\n",
    "where J is the Jacobian matrix with elements $J_{ij} = \\frac{\\partial \\hat{y}_i}{\\partial \\theta_j}$ and incorporates the sigmoid derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.99366568, 3.00319916, 2.00144661])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse_grad(X, y, theta):\n",
    "    n = len(y) \n",
    "    return (-2 / n) * X.T @ (y - (X @ theta))\n",
    "\n",
    "X = np.random.randn(10000, 2)\n",
    "noise = np.random.randn(10000, 1).flatten() * 0.2  # Add noise\n",
    "\n",
    "y = 4 + (X @ np.array([3,2])) + noise\n",
    "\n",
    "X = np.column_stack([np.ones(10000), X])\n",
    "\n",
    "MBGD(X, y, mse_grad, 1000, 0.01, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.56036152, 3.21328784, 2.18231314])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def bce_grad(X, y, theta):\n",
    "    n = len(y) \n",
    "    return (-1 / n) * X.T @ (y - sigmoid(X @ theta))\n",
    "\n",
    "X = np.random.randn(10000, 2)\n",
    "noise = np.random.randn(10000, 1).flatten() * 0.2  # Add noise\n",
    "\n",
    "y = sigmoid(4 + (X @ np.array([3,2])) + noise) \n",
    "y = (y > 0.5).astype(int)\n",
    "X = np.column_stack([np.ones(10000), X])\n",
    "\n",
    "MBGD(X, y, bce_grad, 50, 0.01, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

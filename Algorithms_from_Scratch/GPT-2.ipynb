{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of converting text into smaller units (tokens) that a model can process. The type of tokenization used depends on the model and application:\n",
    "\n",
    "**1. Word Tokenization:**\n",
    "- Splits text into words based on spaces and punctuation.\n",
    "- Example: \"The cat sat.\" → [\"The\", \"cat\", \"sat\", \".\"]\n",
    "- Simple but limited (out-of-vocabulary words cause issues).\n",
    "\n",
    "**2. Character Tokenization:**\n",
    "- Breaks text into individual characters.\n",
    "- Example: \"Hello\" → [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
    "- Good for languages with complex morphology (e.g., Chinese) but inefficient for long texts.\n",
    "\n",
    "**3. Byte Pair Encoding (BPE) Tokenization:**\n",
    "- A subword tokenization method.\n",
    "- Common words are kept as full tokens, while rare words are split into smaller subword units.\n",
    "- Example: \"unhappiness\" → [\"un\", \"happiness\"] (if \"happiness\" is frequent)\n",
    "- GPT-2 uses BPE tokenization, balancing vocabulary size and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'cat', 'is', 'unhappy']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "text = 'The cat is unhappy'\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'c',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'u',\n",
       " 'n',\n",
       " 'h',\n",
       " 'a',\n",
       " 'p',\n",
       " 'p',\n",
       " 'y']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def char_tokenize(text):\n",
    "    return list(text)\n",
    "\n",
    "text = 'The cat is unhappy'\n",
    "char_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buid_vocab(text):\n",
    "    words = text.split()  # Split by spaces\n",
    "    words+=[\"UNK\"]  # Add UNK token\n",
    "    vocab = sorted(set(words))  # Unique words sorted\n",
    "    word_to_id = {word: idx for idx, word in enumerate(vocab)} # create a dict\n",
    "    id_to_word = {idx: word for word, idx in word_to_id.items()} # reverse the dict\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def encode(text, word_to_id):\n",
    "    words = text.split()  # Split by spaces\n",
    "    unknown_id = word_to_id.get(\"UNK\")\n",
    "    return [word_to_id.get(w, unknown_id) for w in words]\n",
    "\n",
    "def decode(ids, id_to_word):\n",
    "    return [id_to_word.get(i, \"UNK\") for i in ids]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hello,': 0, 'How': 1, 'UNK': 2, 'are': 3, 'you': 4}\n",
      "{0: 'Hello,', 1: 'How', 2: 'UNK', 3: 'are', 4: 'you'}\n"
     ]
    }
   ],
   "source": [
    "d1, d2 = buid_vocab(\"Hello, How are you\")\n",
    "print(d1)\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 4, 2, 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"Hello, are you George ?\", d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'are', 'you', 'UNK', 'UNK']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([0, 3, 4, 2, 2], d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings + Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "vocab_size = 10000\n",
    "embed_dim = 768  \n",
    "max_seq_len = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings = np.random.randn(vocab_size, embed_dim)\n",
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 768)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_embeddings = np.random.randn(max_seq_len, embed_dim)\n",
    "positional_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 768)\n",
      "(5, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.61887313,  0.09400312, -3.32888607, ..., -0.21328245,\n",
       "         0.57585842, -0.00361055],\n",
       "       [-0.55717445, -2.77855797, -0.98569963, ..., -1.24417084,\n",
       "        -0.41394384,  3.50313267],\n",
       "       [ 1.56149326,  1.90120072, -0.33256547, ..., -0.98567718,\n",
       "        -0.37131668,  0.12742174],\n",
       "       [-0.33594186, -1.47447108,  2.3595145 , ...,  1.75077137,\n",
       "         0.91849738,  0.17863543],\n",
       "       [-1.93755778, -0.12762102,  0.20301741, ..., -0.63949476,\n",
       "        -1.55418361,  1.94200523]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Hello, how are you doing\"\n",
    "input_ids = encode(input_text, d1)\n",
    "seq_len = len(input_ids)\n",
    "\n",
    "word_emb = word_embeddings[input_ids]\n",
    "pos_emb = positional_embeddings[:seq_len]\n",
    "\n",
    "print(word_emb.shape)\n",
    "print(pos_emb.shape)\n",
    "\n",
    "final_emb = word_emb + pos_emb\n",
    "final_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Self-Attention (MHA) with Causal Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Head Self-Attention (MHA) allows the model to learn different relationships between words. GPT-2 uses *causal attention*, meaning each token can only attend to past tokens (not future ones).\n",
    "\n",
    "Here's a breakdown of how it works:\n",
    "\n",
    "1. **Linear Projections:**\n",
    "   - The input embeddings are transformed into three matrices: Query (Q), Key (K), and Value (V).  These are created by multiplying the input embeddings by learned weight matrices.\n",
    "\n",
    "2. **Scaled Dot-Product Attention:**\n",
    "   - Attention scores are calculated using the following formula:\n",
    "\n",
    "     $$\n",
    "     Attention(Q, K, V) = softmax( (QKᵀ / √dₖ) + mask ) V\n",
    "     $$\n",
    "\n",
    "     Where:\n",
    "     - `Q`, `K`, and `V` are the Query, Key, and Value matrices.\n",
    "     - `dₖ` is the dimension of the Key vectors.  Scaling by the square root of `dₖ` helps stabilize training.\n",
    "     - `mask` is the causal mask.\n",
    "\n",
    "3. **Causal Mask:**\n",
    "   - The mask is crucial for causal attention. It prevents each token from attending to future tokens.  The mask effectively sets the attention scores for future tokens to a very large negative number (often -∞), which, after the softmax operation, makes their attention weights close to zero.\n",
    "\n",
    "4. **Multi-Head Processing:**\n",
    "   - The Q, K, and V matrices are split into multiple \"heads.\" Each head performs the scaled dot-product attention calculation independently. This allows the model to capture different types of relationships between words in parallel.\n",
    "\n",
    "5. **Final Linear Projection:**\n",
    "   - The outputs from all the heads are concatenated.  This concatenated output is then projected back to the original embedding dimension using another learned weight matrix.  This final projection combines the information learned by the different attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = final_emb\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention(Q, K, V, mask):\n",
    "    attn_scores = (Q @ K.T) / np.sqrt(Q.shape[1]) + mask\n",
    "    attn_scores = np.exp(attn_scores) / np.sum(np.exp(attn_scores), axis=-1, keepdims=True)\n",
    "    return attn_scores @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 768)\n",
      "(5, 768)\n",
      "(5, 768)\n",
      "(5, 12, 64)\n",
      "(5, 12, 64)\n",
      "(5, 12, 64)\n",
      "(12, 5, 64)\n",
      "(5, 768)\n",
      "(5, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j2/2p1vsnfj2kn0wrq64fqdvds448_r7g/T/ipykernel_3108/958733516.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  attn_scores = np.exp(attn_scores) / np.sum(np.exp(attn_scores), axis=-1, keepdims=True)\n",
      "/var/folders/j2/2p1vsnfj2kn0wrq64fqdvds448_r7g/T/ipykernel_3108/958733516.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  attn_scores = np.exp(attn_scores) / np.sum(np.exp(attn_scores), axis=-1, keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "# Example Usage:\n",
    "# assert embed_dim % num_heads == 0, \"K must be divisible by number of heads\"\n",
    "embed_dim = 768\n",
    "num_heads = 12\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "seq_len = x.shape[0] # length if input sequqnce. (number of tokens in input)\n",
    "\n",
    "W_q = np.random.randn(embed_dim, embed_dim) \n",
    "W_k = np.random.randn(embed_dim, embed_dim)\n",
    "W_v = np.random.randn(embed_dim, embed_dim) \n",
    "W_o = np.random.randn(embed_dim, embed_dim) \n",
    "\n",
    "# x: (seq_len, embed_dim) - Input embeddings\n",
    "# Compute Q, K, V\n",
    "Q = x @ W_q  # Shape: (seq_len, embed_dim)\n",
    "K = x @ W_k\n",
    "V = x @ W_v\n",
    "\n",
    "# Compute Q, K, V\n",
    "print(Q.shape)\n",
    "print(K.shape)\n",
    "print(V.shape)\n",
    "\n",
    "causal_mask = np.triu(np.ones((seq_len, seq_len)) * -1e9, k=1)\n",
    "# print(causal_mask)\n",
    "\n",
    "Q = Q.reshape(seq_len, num_heads, head_dim)\n",
    "K = K.reshape(seq_len, num_heads, head_dim)\n",
    "V = V.reshape(seq_len, num_heads, head_dim)\n",
    "print(Q.shape) # Now these are 12 matrices of size 5x64. (num_heads matrices of dimension seq_len x  head_dim)\n",
    "print(K.shape)\n",
    "print(V.shape)\n",
    "\n",
    "# Compute attention for each head\n",
    "attention_outputs = np.stack([   get_attention(Q[:,i,:], K[:,i,:], V[:,i,:], causal_mask) for i in range(num_heads)  ])\n",
    "print(attention_outputs.shape)\n",
    "output = attention_outputs.transpose(1, 0, 2).reshape(seq_len, embed_dim)\n",
    "print(output.shape)\n",
    "\n",
    "output = output @ W_o\n",
    "\n",
    "print(output.shape)  # Should be (seq_len, embed_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Norm + Residual Connections + Fully Connected NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, gamma, beta, eps=1e-5):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    variance = np.var(x, axis=-1, keepdims=True)\n",
    "    return gamma * (x - mean) / np.sqrt(variance + eps) + beta\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def linear(x, W, b):\n",
    "    return x @ W + b\n",
    "\n",
    "def feed_forward(x, W1, b1, W2, b2):\n",
    "    return linear(gelu(linear(x, W1, b1)), W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (learned during training)\n",
    "gamma = np.ones(embed_dim)  # Scale parameter for LayerNorm\n",
    "beta = np.zeros(embed_dim)  # Shift parameter for LayerNorm\n",
    "\n",
    "W1 = np.random.randn(embed_dim, 4 * embed_dim)  # First FC layer weights\n",
    "b1 = np.zeros(4 * embed_dim)  # First FC layer bias\n",
    "W2 = np.random.randn(4 * embed_dim, embed_dim)  # Second FC layer weights\n",
    "b2 = np.zeros(embed_dim)  # Second FC layer bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 768)\n",
      "(5, 768)\n"
     ]
    }
   ],
   "source": [
    "# Residual Connection after Multi-Head Attention\n",
    "print(x.shape) # input_embeddings  # (seq_len, embed_dim)\n",
    "print(output.shape) # attention_output = multi_head_attention(x)  # (seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_residual_plus_layer_norm = layer_norm(x + output, gamma, beta)  # Apply residual + LayerNorm \n",
    "output_residual_plus_layer_norm.shape # (seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn_output = feed_forward(output_residual_plus_layer_norm, W1, b1, W2, b2)  # (seq_len, embed_dim)\n",
    "ffn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Residual Connection after Feed-Forward Network\n",
    "output_another_layer_norm = layer_norm(x + ffn_output, gamma, beta)\n",
    "output_another_layer_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = output_another_layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10000)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = H @ word_embeddings.T \n",
    "logits.shape # Shape: (seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token = np.argmax(probs[-1])\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Gradient Descent + Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is an optimization algorithm used to minimize a function by iteratively updating its parameters in the opposite direction of the gradient.\n",
    "\n",
    "**Formula**\n",
    "\n",
    "Given a function \\( J(\\theta) \\) that we want to minimize, the update rule for gradient descent is:\n",
    "\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\nabla J(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "def SGD(X, y, gradient_function, num_epochs = 100, lr = 0.01, tol =1e-4):\n",
    "    num_rows, num_colmns = X.shape\n",
    "    theta = np.random.randn(num_colmns)\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "        for i in range(num_rows):\n",
    "            idx = np.random.randint(num_rows)  # Pick a random sample\n",
    "            xi = X[idx]\n",
    "            yi = y[idx]\n",
    "            grad = gradient_function(xi, yi, theta)\n",
    "            theta = theta - (lr * grad)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "def SGD(X, y, gradient_function, num_epochs = 100, lr = 0.01, tol =1e-4):\n",
    "    num_rows, num_colmns = X.shape\n",
    "    theta = np.random.randn(num_colmns)\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "        sum_grad = np.zeros(num_colmns)\n",
    "\n",
    "        for i in range(num_rows):\n",
    "            idx = np.random.randint(num_rows)  # Pick a random sample\n",
    "            xi = X[idx]\n",
    "            yi = y[idx]\n",
    "            grad = gradient_function(xi, yi, theta)\n",
    "            theta = theta - (lr * grad)\n",
    "\n",
    "            sum_grad += grad\n",
    "\n",
    "        # Check gradient norm after an epoch\n",
    "        if np.linalg.norm(sum_grad / num_rows) < tol:\n",
    "            print(\"Convergence reached.\")\n",
    "            break\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# Generate dummy data\n",
    "np.random.seed(42)\n",
    "num_rows = 10000\n",
    "\n",
    "# Define the true relationship: y = 4 + 3x + noise\n",
    "true_intercept = 4\n",
    "true_slope = 3\n",
    "noise = np.random.randn(num_rows)\n",
    "\n",
    "X = np.random.rand(num_rows) # Single feature\n",
    "y = true_intercept + true_slope * X + noise\n",
    "\n",
    "X = np.column_stack([np.ones(num_rows), X])\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned parameters: [3.92005662 2.94978152]\n"
     ]
    }
   ],
   "source": [
    "def mse_gradient(xi, yi, theta):\n",
    "    return -2 * xi * (yi - np.dot(xi, theta))\n",
    "\n",
    "theta = SGD(X, y, mse_gradient, lr=0.01, num_epochs=100)\n",
    "print(\"Learned parameters:\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost Function**\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "Where:\n",
    "\n",
    "*   $J(\\theta)$ is the cost function.\n",
    "*   $y_i$ is the i-th actual output value.\n",
    "*   $\\hat{y_i}$ is the i-th predicted output value ( $h_\\theta(x^{(i)})$ ).\n",
    "*   $m$ is the number of training examples.\n",
    "*   The predicted output is calculated as the dot product of the parameter vector $\\theta$ and the input vector $x_i$:\n",
    "\n",
    "The goal is to find the values of $\\theta$ that minimize $J(\\theta)$.\n",
    "\n",
    "----\n",
    "\n",
    "**Gradient**\n",
    "\n",
    "The gradient of the MSE cost function with respect to $\\theta_j$ is:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} -2x_i(y_i - \\hat{y_i})$$\n",
    "\n",
    "Where:\n",
    "\n",
    "*   $x_i$ is the i-th input feature (or 1 for the bias term)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent + Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Understanding the Logistic Model\n",
    "\n",
    "Logistic regression predicts probabilities using the sigmoid function:\n",
    "\n",
    "$$\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "where $z$ is the linear combination of inputs:\n",
    "\n",
    "$$z = x_i \\cdot \\theta$$\n",
    "\n",
    "Here:\n",
    "\n",
    "*   $x_i$ is the feature vector for a single training sample.\n",
    "*   $\\theta$ is the weight vector.\n",
    "*   $\\hat{y}$ is the predicted probability for class 1.\n",
    "\n",
    "### 2. Loss Function: Binary Cross-Entropy (Log-Loss)\n",
    "\n",
    "The cost function used in logistic regression is the log-loss:\n",
    "\n",
    "$$J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$$\n",
    "\n",
    "where:\n",
    "\n",
    "*   $y_i$ is the actual class label (0 or 1).\n",
    "*   $\\hat{y}_i$ is the predicted probability.\n",
    "*   $m$ is the number of training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gradient of the Log-Loss Function (Single Data Point)\n",
    "\n",
    "Here's a breakdown of how to derive the gradient of the binary cross-entropy loss function (log-loss) for a single data point.\n",
    "\n",
    "**1. The Setup**\n",
    "\n",
    "For a single data point *i*, the loss function is:\n",
    "\n",
    "$$J_i(\\theta) = - [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]$$\n",
    "\n",
    "Where:\n",
    "\n",
    "*   $y_i$ is the true label (0 or 1).\n",
    "*   $\\hat{y}_i = \\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the predicted probability (sigmoid function).\n",
    "*   $z = x_i \\cdot \\theta = x_i^T \\theta$ is the linear combination of features and weights.\n",
    "\n",
    "**2. The Goal**\n",
    "\n",
    "We want to find the gradient of $J_i(\\theta)$ with respect to the weights $\\theta$, i.e., $\\frac{\\partial J_i(\\theta)}{\\partial \\theta_j}$ for each $\\theta_j$.\n",
    "\n",
    "**3. The Chain Rule**\n",
    "\n",
    "We'll use the chain rule: If $f(g(h(x)))$, then $\\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dh} \\cdot \\frac{dh}{dx}$.  In our case:\n",
    "\n",
    "$$\\frac{\\partial J_i}{\\partial \\theta_j} = \\frac{\\partial J_i}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial z} \\cdot \\frac{\\partial z}{\\partial \\theta_j}$$\n",
    "\n",
    "**4. Breaking it Down**\n",
    "\n",
    "*   **a) $\\frac{\\partial J_i}{\\partial \\hat{y}_i}$:**\n",
    "\n",
    "    $$\\frac{\\partial J_i}{\\partial \\hat{y}_i} = - \\left[ \\frac{y_i}{\\hat{y}_i} - \\frac{1 - y_i}{1 - \\hat{y}_i} \\right] = \\frac{\\hat{y}_i - y_i}{\\hat{y}_i(1 - \\hat{y}_i)}$$\n",
    "\n",
    "*   **b) $\\frac{\\partial \\hat{y}_i}{\\partial z}$:**\n",
    "\n",
    "    $$\\frac{\\partial \\hat{y}_i}{\\partial z} = \\frac{\\partial \\sigma(z)}{\\partial z} = \\sigma(z)(1 - \\sigma(z)) = \\hat{y}_i(1 - \\hat{y}_i)$$\n",
    "\n",
    "*   **c) $\\frac{\\partial z}{\\partial \\theta_j}$:**\n",
    "\n",
    "    $$\\frac{\\partial z}{\\partial \\theta_j} = \\frac{\\partial (x_i^T \\theta)}{\\partial \\theta_j} = x_{ij}$$ \n",
    "    \n",
    "    (The *j*-th feature of the *i*-th data point).\n",
    "\n",
    "**5. Putting it Together**\n",
    "\n",
    "Substituting the derivatives into the chain rule equation:\n",
    "\n",
    "$$\\frac{\\partial J_i}{\\partial \\theta_j} = \\left( \\frac{\\hat{y}_i - y_i}{\\hat{y}_i(1 - \\hat{y}_i)} \\right) \\cdot \\left( \\hat{y}_i(1 - \\hat{y}_i) \\right) \\cdot x_{ij}$$\n",
    "\n",
    "The $\\hat{y}_i(1 - \\hat{y}_i)$ terms cancel out.\n",
    "\n",
    "**6. The Final Gradient**\n",
    "\n",
    "$$\\frac{\\partial J_i}{\\partial \\theta_j} = (\\hat{y}_i - y_i) x_{ij}$$\n",
    "\n",
    "The gradient vector for the entire $\\theta$ is:\n",
    "\n",
    "$$\\nabla J_i(\\theta) = (\\hat{y}_i - y_i) x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same SGD can be utilised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_gradient(xi, yi, theta):\n",
    "    yi_hat = sigmoid(np.dot(xi, theta))\n",
    "    grad = (yi_hat - yi)*xi\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 10000\n",
    "X = np.random.randn(num_rows) # Single feature\n",
    "X = np.column_stack((np.ones(num_rows), X)) # Add bias term\n",
    "true_thetas = np.array([4, 5])\n",
    "\n",
    "noise = np.random.randn(num_rows)\n",
    "\n",
    "y = sigmoid(np.dot(X, true_thetas) + noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached.\n",
      "[3.49792462 4.36483606]\n"
     ]
    }
   ],
   "source": [
    "predicted_thetas = SGD(X, y, logistic_gradient, num_epochs = 1000, lr = 0.01)\n",
    "print(predicted_thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch / Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MBGD(X, y, gradient_function, batch_size = 32, num_epochs = 100, lr = 0.01):\n",
    "    \n",
    "    num_rows, num_cols = X.shape\n",
    "    thetas = np.random.randn(num_cols)\n",
    "\n",
    "    for i in range(0, num_rows, batch_size):\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y_batch = y[i:i+batch_size]\n",
    "\n",
    "        batch_grad = np.mean(gradient_function(X_batch, y_batch, thetas))\n",
    "\n",
    "        thetas = thetas - (lr*batch_grad)\n",
    "\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reg_gradient(X, y, theta):\n",
    "    grad = -2 * np.dot(X.T, (y - np.dot(X, theta)))\n",
    "    return grad/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# Generate dummy data\n",
    "np.random.seed(42)\n",
    "num_rows = 10000\n",
    "\n",
    "# Define the true relationship: y = 4 + 3x + noise\n",
    "true_intercept = 4\n",
    "true_slope = 3\n",
    "noise = np.random.randn(num_rows)\n",
    "\n",
    "X = np.random.rand(num_rows) # Single feature\n",
    "y = true_intercept + true_slope * X + noise\n",
    "\n",
    "X = np.column_stack([np.ones(num_rows), X])\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.75084237, 3.34222619])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MBGD(X, y, linear_reg_gradient, batch_size = 32, num_epochs = 1000, lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

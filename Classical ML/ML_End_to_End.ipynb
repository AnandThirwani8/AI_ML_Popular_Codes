{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate dummy dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 10000\n",
    "\n",
    "# Create features\n",
    "numerical_features = np.random.randn(n_samples, 3) * 10\n",
    "numerical_features[np.random.randint(0, n_samples, 500), np.random.randint(0, 3, 500)] = np.nan  # Missing values\n",
    "\n",
    "categories = ['A', 'B', 'C', 'D', 'E']\n",
    "categorical_features = np.random.choice(categories, n_samples)\n",
    "categorical_features[np.random.randint(0, n_samples, 300)] = np.nan  # Missing values\n",
    "\n",
    "text_data = np.random.choice([\"This is a sample text\", \"Another text data\", \"More random text data\"], n_samples)\n",
    "\n",
    "# Imbalanced target variable\n",
    "target_classes = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\n",
    "class_distribution = [0.7, 0.2, 0.08, 0.02]\n",
    "target = np.random.choice(target_classes, n_samples, p=class_distribution)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(numerical_features, columns=[\"num_1\", \"num_2\", \"num_3\"])\n",
    "df['category'] = categorical_features\n",
    "df['text'] = text_data\n",
    "df['target'] = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-test split\n",
    "X, y = df.drop(columns=['target']), df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Impute missing values\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# One-Hot Encoding with 'Other' category handling\n",
    "ohe = OneHotEncoder(handle_unknown='infrequent_if_exist', min_frequency=0.01)  # Less frequent categories go to 'other'\n",
    "\n",
    "# Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=100)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Column Transformer\n",
    "# Each transformation is defined as a tuple: ('name', transformer, columns)\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numerical_transformation', Pipeline([('imputer', num_imputer), ('scaler', scaler)]), ['num_1', 'num_2', 'num_3']),\n",
    "    ('categorical_transformation', Pipeline([('imputer', cat_imputer), ('ohe', ohe)]), ['category']),\n",
    "    ('text_transformation', tfidf, 'text')\n",
    "])\n",
    "\n",
    "# Apply transformations\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ColumnTransformer applies different transformations to different types of columns in the dataset.\n",
    "- Each transformation is defined as a tuple: ('name', transformer, columns), where:\n",
    "  \n",
    "  - name: A label for the transformation step.\n",
    "  - transformer: The actual transformation pipeline (e.g., Pipeline, TfidfVectorizer).\n",
    "  - columns: The specific columns this transformation applies to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "pred = model.predict(X_test_transformed)\n",
    "\n",
    "report = classification_report(y_test, pred)\n",
    "f1_macro = f1_score(y_test, pred, average='macro')\n",
    "\n",
    "# Print classification reports\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"F1 Macro Score:\", f1_macro)\n",
    "print(\"--\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "input_shape = X_train_smote.shape[1]\n",
    "output_shape = len(np.unique(y_train))\n",
    "print(input_shape)\n",
    "print(output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_map = {'Class_1':0, 'Class_2':1, 'Class_3':2, 'Class_4':3}\n",
    "y_train_smote_ids = np.array([dict_map[y] for y in y_train_smote])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN Model\n",
    "inputs = Input(shape=(input_shape,))\n",
    "\n",
    "x = layers.Dense(64)(inputs)  # No activation yet\n",
    "x = layers.BatchNormalization()(x)  # Normalize before activation\n",
    "x = layers.ReLU()(x)  # Apply activation\n",
    "x = layers.Dropout(0.2)(x)  # Dropout AFTER activation\n",
    "\n",
    "x = layers.Dense(32, activation='relu')(x) # just to show that relu can also be applied here, but it is less efficient\n",
    "x = layers.BatchNormalization()(x) \n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "outputs = layers.Dense(output_shape, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy' , metrics = ['accuracy'])\n",
    "# model.compile(optimizer='adam', loss='mse' , metrics = ['mae']) # Fore regression\n",
    "\n",
    "# Train model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)              \n",
    "history = model.fit(X_train_smote, y_train_smote_ids, epochs=3, batch_size=16, verbose=1, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_map_inverse = {v:k for k, v in dict_map.items()}\n",
    "dict_map_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([dict_map_inverse[y] for y in y_pred])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "f1_macro = f1_score(y_test, pred, average='macro')\n",
    "\n",
    "# Print classification reports\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"F1 Macro Score:\", f1_macro)\n",
    "print(\"--\"*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

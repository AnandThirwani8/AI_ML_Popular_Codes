{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Define folder paths\n",
    "output_folder = \"mnist_sample_data\"\n",
    "train_folder = os.path.join(output_folder, \"train\")\n",
    "test_folder = os.path.join(output_folder, \"test\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "# Set sample size\n",
    "train_sample_size = 500  # Number of training images\n",
    "test_sample_size = 500   # Number of testing images\n",
    "\n",
    "# Save a subset of training images\n",
    "for i, (image, label) in enumerate(zip(x_train[:train_sample_size], y_train[:train_sample_size])):\n",
    "    label_folder = os.path.join(train_folder, str(label))\n",
    "    os.makedirs(label_folder, exist_ok=True)\n",
    "    image_path = os.path.join(label_folder, f\"{i}.png\")\n",
    "    Image.fromarray(image).save(image_path)\n",
    "\n",
    "# Save a subset of test images\n",
    "for i, (image, label) in enumerate(zip(x_test[:test_sample_size], y_test[:test_sample_size])):\n",
    "    label_folder = os.path.join(test_folder, str(label))\n",
    "    os.makedirs(label_folder, exist_ok=True)\n",
    "    image_path = os.path.join(label_folder, f\"{i}.png\")\n",
    "    Image.fromarray(image).save(image_path)\n",
    "\n",
    "print(f\"MNIST sample dataset (1000 images) has been saved in '{output_folder}' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from a Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define data paths\n",
    "train_dir = \"mnist_sample_data/train\"\n",
    "test_dir = \"mnist_sample_data/test\"\n",
    "\n",
    "# Define hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (128, 128)  # Resize images for transfer learning\n",
    "\n",
    "# Load training and validation datasets\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"int\"\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"int\"\n",
    ")\n",
    "\n",
    "# Data Augmentation\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2),\n",
    "])\n",
    "\n",
    "# Apply prefetching for performance optimization\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y)).prefetch(AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MobileNetV2 as a base model (Transfer Learning)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SIZE + (3,), \n",
    "                                               include_top=False,  # Remove fully connected layers\n",
    "                                               weights=\"imagenet\")\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "# Build the Transfer Learning Model\n",
    "# Define Input Layer\n",
    "inputs = keras.Input(shape=(128, 128, 3))\n",
    "x = base_model(inputs, training=False)  # Apply Pretrained Model # Ensure batch norm layers don't update\n",
    "x = layers.GlobalAveragePooling2D()(x) # Global Average Pooling\n",
    "x = layers.Dense(128, activation=\"relu\")(x) # Fully Connected Layers\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x) # Output Layer (10 Classes)\n",
    "\n",
    "# Create Model\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_ds, validation_data=test_ds, epochs=3)\n",
    "\n",
    "# Plot training results\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Accuracy vs Validation Accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for _, y in test_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_ds)\n",
    "y_pred = np.argmax(y_pred, axis = 1)\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "report = classification_report(y_test, y_pred)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Print classification reports\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"F1 Macro Score:\", f1_macro)\n",
    "print(\"--\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
